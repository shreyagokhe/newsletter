{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:playlist] Downloading just video Xv8Abp_1GaM because of --no-playlist\n",
      "[youtube] Xv8Abp_1GaM: Downloading webpage\n",
      "[youtube] Downloading just video Xv8Abp_1GaM because of --no-playlist\n",
      "[youtube] Xv8Abp_1GaM: Downloading MPD manifest\n",
      "[dashsegments] Total fragments: 89\n",
      "[download] Destination: Xv8Abp_1GaM.m4a\n",
      "[download] 100% of 13.52MiB in 00:18                    \n",
      "[ffmpeg] Correcting container in \"Xv8Abp_1GaM.m4a\"\n",
      "[ffmpeg] Destination: Xv8Abp_1GaM.wav\n",
      "Deleting original file Xv8Abp_1GaM.m4a (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals #you are building a byte string that holds UTF-8 encoded bytes.\n",
    "                                        #With the string you are building a unicode string.\n",
    "import youtube_dl #Command-line program to download videos from YouTube.com and other video sites\n",
    "\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',     #getting the best format\n",
    "    'extractaudio':True,            #only extract audio\n",
    "    'audioformat':'wav',            #save audio in .wav format-- easy to convert as most recordings are in .wav\n",
    "    'outtmpl':'%(id)s.%(ext)s',     #name the file as the ID of the video\n",
    "    'noplaylist':True,              #no playlist download\n",
    "    'nocheckcertificate':True,       \n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio', #asks to extract audio\n",
    "        'preferredcodec': 'wav',     #file type\n",
    "        'preferredquality': '192',   #quality index\n",
    "    }],\n",
    "}\n",
    "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download(['https://www.youtube.com/watch?v=cO3QvioMESo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                #provides functions for interacting with the operating system\n",
    "import speech_recognition as sr     #library to perform speech recognition\n",
    "\n",
    "from pydub import AudioSegment      #, if there are several overlapping sounds in this AudioSegment,this \n",
    "                                     #method will return one AudioSegment object for each of those sounds. \n",
    "from pydub.silence import split_on_silence     #if there is silence in the vid, then we will split the sound\n",
    "\n",
    "r = sr.Recognizer()           # a function that splits the audio file into chunks\n",
    "                                    # and applies speech recognition\n",
    "                                \n",
    "def get_large_audio_transcription(path):\n",
    "    \"\"\"\n",
    "    Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    sound = AudioSegment.from_wav(path)   # open the audio file using pydub\n",
    "    \n",
    "    fh = open(\"converted.txt\", \"w+\")  #open the file to write\n",
    "    chunks = split_on_silence(sound,   # split audio sound where silence is 500 miliseconds or more and get chunks\n",
    "       \n",
    "        min_silence_len = 500,    # experiment with this value for your target audio file\n",
    "        \n",
    "        silence_thresh = sound.dBFS-14,  # adjust this per requirement\n",
    "        \n",
    "        keep_silence=500,   # keep the silence for 1 second, adjustable as well\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"     # create a directory to store the audio chunks\n",
    "   \n",
    "    if not os.path.isdir(folder_name):    # asking os to create such foldername if not present and passing the text \n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):     # export audio chunk and save it in\n",
    "                                                         # the `folder_name` directory.\n",
    "        \n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")       # recognize the chunk\n",
    "       \n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)               # try converting it to text\n",
    "            \n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened)\n",
    "                fh.write(text+\". \")      #writing in the .txt file\n",
    "            except sr.UnknownValueError as e:\n",
    "                print(\"Error:\", str(e))\n",
    "            else:\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_filename, \":\", text)\n",
    "                whole_text += text\n",
    "                                         # return the text for all chunks detected\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-chunks\\chunk1.wav : Sometimes if i'm going to catch a flight i actually do use an alarm clock. \n",
      "audio-chunks\\chunk2.wav : So tired i'm playing and i'll be presenting i'll have what she's having and other models of behavior. \n",
      "audio-chunks\\chunk3.wav : Some background about me i started my career at near economic consulting in the securities practice. \n",
      "audio-chunks\\chunk4.wav : Estimating damages and analyzing claims for using litigation. \n",
      "audio-chunks\\chunk5.wav : This range from. \n",
      "audio-chunks\\chunk6.wav : Topix. \n",
      "audio-chunks\\chunk7.wav : Such as. \n",
      "audio-chunks\\chunk8.wav : The fallout from the financial crisis to product liability to predatory lending. \n",
      "audio-chunks\\chunk9.wav : And more recently i started at chop't creative salad company and strategy and analytics. \n",
      "audio-chunks\\chunk10.wav : And this is my first time going to work with consumers and consumer data so i really wanted to get a better sense of some of the trends in the industry and some of the anxieties that we're all facing today because i'm not only. \n",
      "audio-chunks\\chunk11.wav : Like a data scientist i'm also a person a user and the datapoint. \n",
      "audio-chunks\\chunk12.wav : So i wanted to be a little bit more cognizant about how i was thinking about that. \n",
      "audio-chunks\\chunk13.wav : So first we're going to talk about what some models of customer consumer behavior would be. \n",
      "audio-chunks\\chunk14.wav : And then i'll. \n",
      "audio-chunks\\chunk15.wav : Tie that to some of my experience working with. \n",
      "audio-chunks\\chunk16.wav : Working with barbara behavior and some of the intertwined history of discrimination there and also just exploring the facebook platform because together with google that accounts for 60% of market share for digital ads in the us with i thought that was a pretty good place to start and also their website apparently has a lot of good hidden fun facts about. \n",
      "audio-chunks\\chunk17.wav : Some of the trends going on. \n",
      "audio-chunks\\chunk18.wav : So by prince predictive customer behavior modeling. \n",
      "audio-chunks\\chunk19.wav : Ultimately i think what organizations are trying to do is segment customers based on their behavior and track them overtime and letting their purchase behavior in order to understand if this is going to be a lucrative customer going forward especially relative to the customer acquisition cost. \n",
      "audio-chunks\\chunk20.wav : And ultimately it's designed to make a yes no decision about whether or not you're going to engage with that customer. \n",
      "audio-chunks\\chunk21.wav : This reminded me a lot of. \n",
      "audio-chunks\\chunk22.wav : Models of fire behavior we're using different factors that borrow represents in the loan application and that ultimately determines whether or not you think that a borrower is going to repay a loan. \n",
      "audio-chunks\\chunk23.wav : Some factors may include credit score loan to value debt to income documentation employment history. \n",
      "audio-chunks\\chunk24.wav : And ultimately the bank uses those factors to decide whether or not. \n",
      "audio-chunks\\chunk25.wav : Whether or not you're eligible for a loan. \n",
      "audio-chunks\\chunk26.wav : And if so what. \n",
      "audio-chunks\\chunk27.wav : What they would charge you for it. \n",
      "audio-chunks\\chunk28.wav : Notably absent on this list are raised familial status another protected categories. \n",
      "audio-chunks\\chunk29.wav : But i was going forward were becoming more involved with social media you could use factors outside of these outside of these established in. \n",
      "audio-chunks\\chunk30.wav : Loan application factors. \n",
      "audio-chunks\\chunk31.wav : To predict things going forward. \n",
      "audio-chunks\\chunk32.wav : Also banks actually pull these loans together across a variety of factors which i think makes sense because if you're relying too much on. \n",
      "audio-chunks\\chunk33.wav : 1 concentrated exposure for example all coastal. \n",
      "audio-chunks\\chunk34.wav : People who want beachfront property are all people who work for the acme car company if there is a bad climate event or bad weather event or. \n",
      "audio-chunks\\chunk35.wav : There's. \n",
      "audio-chunks\\chunk36.wav : Sales slump in cars. \n",
      "audio-chunks\\chunk37.wav : Then this could have a negative impact to their bottom line. \n",
      "audio-chunks\\chunk38.wav : And also if there's only one bank or if all the banks are using the same model then customers might. \n",
      "audio-chunks\\chunk39.wav : The limited in their choice or even awareness of the offers. \n",
      "audio-chunks\\chunk40.wav : Which is. \n",
      "audio-chunks\\chunk41.wav : What happened in the history of discrimination redlining. \n",
      "audio-chunks\\chunk42.wav : In the mortgage market where it used to be acceptable to say that specific neighborhoods were considered undesirable or risky and this led to selective redlining or. \n",
      "audio-chunks\\chunk43.wav : Underinvestment in minority communities. \n",
      "audio-chunks\\chunk44.wav : And that had a vicious cyclical effects wear under investment also. \n",
      "audio-chunks\\chunk45.wav : Prevented them from appreciating assassin value. \n",
      "audio-chunks\\chunk46.wav : And during the civil rights movement the government stepped in. \n",
      "audio-chunks\\chunk47.wav : And. \n",
      "audio-chunks\\chunk48.wav : Through regulation and. \n",
      "audio-chunks\\chunk49.wav : The establishment of new agencies. \n",
      "audio-chunks\\chunk50.wav : Was able to. \n",
      "audio-chunks\\chunk51.wav : Try to combat this with new laws and then crucially in 1975 there was also the home mortgage disclosure act switch from the cfpb website just want to read requires many financial institutions to maintain rapport and publicly disclose loan level information about mortgages. \n",
      "audio-chunks\\chunk52.wav : These data help show whether lenders are serving the housing needs of their communities they give public officials information that helps them make decisions and policies and they sent. \n",
      "audio-chunks\\chunk53.wav : Shed light on landing patterns that can be discriminatory. \n",
      "audio-chunks\\chunk54.wav : The public theater modified to protect applicant and borrow privacy. \n",
      "audio-chunks\\chunk55.wav : I think this is really important because. \n",
      "audio-chunks\\chunk56.wav : And just taking a public example from. \n",
      "audio-chunks\\chunk57.wav : The air package handed data can raise a lot of questions so in one example. \n",
      "audio-chunks\\chunk58.wav : The boston fed looked into differences. \n",
      "audio-chunks\\chunk59.wav : In the rate of denial of loan applications between white and minority borrowers. \n",
      "audio-chunks\\chunk60.wav : And further collection showed that. \n",
      "audio-chunks\\chunk61.wav : Berg. \n",
      "Error: \n",
      "audio-chunks\\chunk63.wav : Seller applications. \n",
      "Error: \n",
      "audio-chunks\\chunk65.wav : For certain. \n",
      "audio-chunks\\chunk66.wav : This part of this result was related to the underlying factors. \n",
      "audio-chunks\\chunk67.wav : But for less than stellar applications that was actually because. \n",
      "audio-chunks\\chunk68.wav : When you need to compensate. \n",
      "audio-chunks\\chunk69.wav : Compensating factors. \n",
      "audio-chunks\\chunk70.wav : It was the benefit of the doubt this more often given to white borrowers. \n",
      "audio-chunks\\chunk71.wav : And because it's such a nuanced and complicated story i think that's why you continue to see litigation this filled it's hard to pick-a-part what's actually an underlying factor in what's actually discriminatory. \n",
      "audio-chunks\\chunk72.wav : And the complaint will often state that someone was discriminated against relative to a similarly situated whitebark which shows that you yourself might not know when you've been discriminated against for targeted until you have a full data set. \n",
      "audio-chunks\\chunk73.wav : And i think that you can see him specific categories in this is. \n",
      "audio-chunks\\chunk74.wav : Just hypothetical if you're just using one. \n",
      "audio-chunks\\chunk75.wav : One category of fico score to predict something and apr is on the y-axis. \n",
      "audio-chunks\\chunk76.wav : Then you're looking to see if you've been given a different apr based on your nearest neighbor or somebody somebody who has the exact same factors as you and every other way and i think future of personalized experiences can make this especially tricky because you might be different in a lot of different ways and it will be hard to see if you've been treated unfairly. \n",
      "audio-chunks\\chunk77.wav : And i think this relates to. \n",
      "audio-chunks\\chunk78.wav : The practice of just allowing people to present the facts and in the gala terry and fashion. \n",
      "audio-chunks\\chunk79.wav : So hard recently. \n",
      "audio-chunks\\chunk80.wav : Dude facebook because they had just been allowing advertisers to target based on different factors which require elated to the protected classes. \n",
      "audio-chunks\\chunk81.wav : And. \n",
      "audio-chunks\\chunk82.wav : Unfortunately also drawing red lines around a specific geographies which she can see would be problematic. \n",
      "audio-chunks\\chunk83.wav : And facebook develops more of its anti-discriminatory policy and said for housing employment in credit they were actually going to move to a new platform and prevent people from using specific protected classes for targeting. \n",
      "audio-chunks\\chunk84.wav : And hopefully that this would help alleviate the problem. \n",
      "audio-chunks\\chunk85.wav : But i think it's also strikes at the heart of the business model because. \n",
      "audio-chunks\\chunk86.wav : If you're reaching people who matter most to you it and you're using all these different factors they can also be correlated with some of those protected classes and it's actually creating a really complicated environment. \n",
      "audio-chunks\\chunk87.wav : And. \n",
      "audio-chunks\\chunk88.wav : I personally have some personal balance between privacy and personalization that i struggle with because sometimes i want to give people my data i don't actually want to. \n",
      "audio-chunks\\chunk89.wav : Get ads that weren't targeted for me and i actually enjoy when there's some personalized discovery and i find new products that i find relevant. \n",
      "audio-chunks\\chunk90.wav : And i think that it's also better use of resources overall when people aren't. \n",
      "audio-chunks\\chunk91.wav : Being forced to watch ads that they don't like. \n",
      "audio-chunks\\chunk92.wav : So it's definitely a balance. \n",
      "audio-chunks\\chunk93.wav : But i also don't want to live in a world where i feel like my choices are actually being predetermined for me. \n",
      "audio-chunks\\chunk94.wav : Based on who i am. \n",
      "audio-chunks\\chunk95.wav : Can i think that as. \n",
      "audio-chunks\\chunk96.wav : People are targeting people based on who they think that we are like. \n",
      "audio-chunks\\chunk97.wav : It could also mean that advertisers are buying for the same users or that everybody's kind of using the same model in the same information in order to push people towards this or that and ultimately it could result in hurting of people are hurting of companies and both ways i think when people are too focused and being pushed in one singular direction. \n",
      "audio-chunks\\chunk98.wav : You might not think about where you're going. \n",
      "audio-chunks\\chunk99.wav : I'm just taking a little bit more into facebook active users so. \n",
      "audio-chunks\\chunk100.wav : As of q1 we had 243000243 million active users on facebook platform and that made up about $30 and revenue per user. \n",
      "audio-chunks\\chunk101.wav : In terms of advertising revenue and digging in a little bit more if you just make like a facebook business profile you can see some interesting statistics about that. \n",
      "Error: \n",
      "audio-chunks\\chunk103.wav : But you see that the audience is about 200 to 250 million so good. lens up. \n",
      "audio-chunks\\chunk104.wav : I've clicked on about 16/30 days post likes 13 and you can see that across america people like coca-cola. \n",
      "audio-chunks\\chunk105.wav : That seems to make sense pizza hut judicial watch buffalo wild wings. \n",
      "Error: \n",
      "Error: \n",
      "audio-chunks\\chunk108.wav : And i wanted to look at some audiences that i was little bit more familiar with so 26 to 28 year old new york city men and women i noticed that the men were clicking on ads. \n",
      "audio-chunks\\chunk109.wav : Fewer times women were slightly. \n",
      "audio-chunks\\chunk110.wav : Comedian. \n",
      "audio-chunks\\chunk111.wav : On median 350000 to 400,000 women more clicking on ads slightly more than the median for the u.s.. \n",
      "audio-chunks\\chunk112.wav : And if you target just looking at people who also like apple you can see that. \n",
      "audio-chunks\\chunk113.wav : 80%. \n",
      "audio-chunks\\chunk114.wav : Instead of 40% 80% of those people are using iphones. \n",
      "audio-chunks\\chunk115.wav : And i'm on man they're clicking on 22 ads now and women are clicking on 27 ads. \n",
      "audio-chunks\\chunk116.wav : And if you think that says something about iphone users i tried a lot of different things. \n",
      "audio-chunks\\chunk117.wav : Basically just liking anything means are more likely to click on ads. \n",
      "audio-chunks\\chunk118.wav : Which creator because you're already more engaged on the platform or because maybe they're being better targeted to you i don't actually know that and i think that's part of the point. \n",
      "Error: \n",
      "audio-chunks\\chunk120.wav : And then i wanted to do some background on this audience because i don't know any of you or not that many of you. \n",
      "audio-chunks\\chunk121.wav : And it surprised surprised you like our studio in our bloggers that wasn't really that helpful. \n",
      "audio-chunks\\chunk122.wav : Focusing on and on new york city i got to know that 40% of. \n",
      "audio-chunks\\chunk123.wav : Potentially this audience has a graduate-level degree which probably meant i was never going to impress you with any of my modeling experience also likes it academics day and bam. \n",
      "audio-chunks\\chunk124.wav : So. \n",
      "audio-chunks\\chunk125.wav : And i think this is important because as you kind of get a better sense of who people are and what their behavior is going to be you can start to think about pushing them in different directions and just taking a page out of richard thaler nudge and when she's talking about choice architecture and how you kind of structure a default choice. \n",
      "audio-chunks\\chunk126.wav : I struggle to even answer this question assume you know how to arrange food in the cafeteria knowing that you're going to impact how kids eat. \n",
      "audio-chunks\\chunk127.wav : So you can either arrange the food to make the kids best off some ostensibly probably making it healthier for them or tuesday food at random because you hate liability or maybe maximizing sale of items are maximizing profits. \n",
      "audio-chunks\\chunk128.wav : But the hardest one was arranging the food to get kids to pick the same choices they would choose themselves. \n",
      "audio-chunks\\chunk129.wav : It seems like circular logic cuz i just told you that like the order the food matters. \n",
      "audio-chunks\\chunk130.wav : So. \n",
      "audio-chunks\\chunk131.wav : That's a really difficult question to answer. \n",
      "audio-chunks\\chunk132.wav : And because it's difficult i think that's why people organizations would want to just give the control back to you and say you know what like if this stuff is expensive to you if you don't like this then you can go in and like manually adjust things the way that you want them to you can also download your data. \n",
      "audio-chunks\\chunk133.wav : But i think if you don't look at your data and lube are context of how people are targeted you then you don't necessarily have a good sense. \n",
      "audio-chunks\\chunk134.wav : Of. \n",
      "audio-chunks\\chunk135.wav : What your data is being used for. \n",
      "audio-chunks\\chunk136.wav : And so in conclusion i think that modeling behavior is really really difficult and that wasn't because. \n",
      "audio-chunks\\chunk137.wav : And that's not necessarily because i think that modeling is always difficult you can make a lot of different models some good or some bad some better some worse. \n",
      "audio-chunks\\chunk138.wav : But actually just struggling to define the terms as a society what we think is fair what we think is a choice in what we think is neutral it's really difficult and how much we value privacy vs personalization that's really difficult and i don't think that's going to be determined by like a genius it allowed somewhere but it's actually something that we have to discuss and talk about. \n",
      "audio-chunks\\chunk139.wav : And also that impact not intent matters though it's not something that you can hide from. \n",
      "Error: \n",
      "audio-chunks\\chunk141.wav : It's something that you have to be proactive about. \n",
      "audio-chunks\\chunk142.wav : And more data obviously needs more scrutiny because the more you know the more you kind of have a responsibility to act responsibly with that. \n",
      "audio-chunks\\chunk143.wav : And also i feel like. \n",
      "audio-chunks\\chunk144.wav : The current environment is very reactive and defensive about these topics. \n",
      "audio-chunks\\chunk145.wav : I think it's actually really cool to talk about because we're all learning more and more about human behavior and uncovering where things have been potentially unfair how we could potentially make things better. \n",
      "audio-chunks\\chunk146.wav : And so just finishing with this which is a twitter i never knew about before but knowledge is power and now i can be complicated so kudos to people who are comfortably. \n",
      "audio-chunks\\chunk147.wav : Keeping it complicated. \n",
      "audio-chunks\\chunk148.wav : Thank you. \n",
      "Error: \n",
      "\n",
      "Full text: Sometimes if i'm going to catch a flight i actually do use an alarm clock. So tired i'm playing and i'll be presenting i'll have what she's having and other models of behavior. Some background about me i started my career at near economic consulting in the securities practice. Estimating damages and analyzing claims for using litigation. This range from. Topix. Such as. The fallout from the financial crisis to product liability to predatory lending. And more recently i started at chop't creative salad company and strategy and analytics. And this is my first time going to work with consumers and consumer data so i really wanted to get a better sense of some of the trends in the industry and some of the anxieties that we're all facing today because i'm not only. Like a data scientist i'm also a person a user and the datapoint. So i wanted to be a little bit more cognizant about how i was thinking about that. So first we're going to talk about what some models of customer consumer behavior would be. And then i'll. Tie that to some of my experience working with. Working with barbara behavior and some of the intertwined history of discrimination there and also just exploring the facebook platform because together with google that accounts for 60% of market share for digital ads in the us with i thought that was a pretty good place to start and also their website apparently has a lot of good hidden fun facts about. Some of the trends going on. So by prince predictive customer behavior modeling. Ultimately i think what organizations are trying to do is segment customers based on their behavior and track them overtime and letting their purchase behavior in order to understand if this is going to be a lucrative customer going forward especially relative to the customer acquisition cost. And ultimately it's designed to make a yes no decision about whether or not you're going to engage with that customer. This reminded me a lot of. Models of fire behavior we're using different factors that borrow represents in the loan application and that ultimately determines whether or not you think that a borrower is going to repay a loan. Some factors may include credit score loan to value debt to income documentation employment history. And ultimately the bank uses those factors to decide whether or not. Whether or not you're eligible for a loan. And if so what. What they would charge you for it. Notably absent on this list are raised familial status another protected categories. But i was going forward were becoming more involved with social media you could use factors outside of these outside of these established in. Loan application factors. To predict things going forward. Also banks actually pull these loans together across a variety of factors which i think makes sense because if you're relying too much on. 1 concentrated exposure for example all coastal. People who want beachfront property are all people who work for the acme car company if there is a bad climate event or bad weather event or. There's. Sales slump in cars. Then this could have a negative impact to their bottom line. And also if there's only one bank or if all the banks are using the same model then customers might. The limited in their choice or even awareness of the offers. Which is. What happened in the history of discrimination redlining. In the mortgage market where it used to be acceptable to say that specific neighborhoods were considered undesirable or risky and this led to selective redlining or. Underinvestment in minority communities. And that had a vicious cyclical effects wear under investment also. Prevented them from appreciating assassin value. And during the civil rights movement the government stepped in. And. Through regulation and. The establishment of new agencies. Was able to. Try to combat this with new laws and then crucially in 1975 there was also the home mortgage disclosure act switch from the cfpb website just want to read requires many financial institutions to maintain rapport and publicly disclose loan level information about mortgages. These data help show whether lenders are serving the housing needs of their communities they give public officials information that helps them make decisions and policies and they sent. Shed light on landing patterns that can be discriminatory. The public theater modified to protect applicant and borrow privacy. I think this is really important because. And just taking a public example from. The air package handed data can raise a lot of questions so in one example. The boston fed looked into differences. In the rate of denial of loan applications between white and minority borrowers. And further collection showed that. Berg. Seller applications. For certain. This part of this result was related to the underlying factors. But for less than stellar applications that was actually because. When you need to compensate. Compensating factors. It was the benefit of the doubt this more often given to white borrowers. And because it's such a nuanced and complicated story i think that's why you continue to see litigation this filled it's hard to pick-a-part what's actually an underlying factor in what's actually discriminatory. And the complaint will often state that someone was discriminated against relative to a similarly situated whitebark which shows that you yourself might not know when you've been discriminated against for targeted until you have a full data set. And i think that you can see him specific categories in this is. Just hypothetical if you're just using one. One category of fico score to predict something and apr is on the y-axis. Then you're looking to see if you've been given a different apr based on your nearest neighbor or somebody somebody who has the exact same factors as you and every other way and i think future of personalized experiences can make this especially tricky because you might be different in a lot of different ways and it will be hard to see if you've been treated unfairly. And i think this relates to. The practice of just allowing people to present the facts and in the gala terry and fashion. So hard recently. Dude facebook because they had just been allowing advertisers to target based on different factors which require elated to the protected classes. And. Unfortunately also drawing red lines around a specific geographies which she can see would be problematic. And facebook develops more of its anti-discriminatory policy and said for housing employment in credit they were actually going to move to a new platform and prevent people from using specific protected classes for targeting. And hopefully that this would help alleviate the problem. But i think it's also strikes at the heart of the business model because. If you're reaching people who matter most to you it and you're using all these different factors they can also be correlated with some of those protected classes and it's actually creating a really complicated environment. And. I personally have some personal balance between privacy and personalization that i struggle with because sometimes i want to give people my data i don't actually want to. Get ads that weren't targeted for me and i actually enjoy when there's some personalized discovery and i find new products that i find relevant. And i think that it's also better use of resources overall when people aren't. Being forced to watch ads that they don't like. So it's definitely a balance. But i also don't want to live in a world where i feel like my choices are actually being predetermined for me. Based on who i am. Can i think that as. People are targeting people based on who they think that we are like. It could also mean that advertisers are buying for the same users or that everybody's kind of using the same model in the same information in order to push people towards this or that and ultimately it could result in hurting of people are hurting of companies and both ways i think when people are too focused and being pushed in one singular direction. You might not think about where you're going. I'm just taking a little bit more into facebook active users so. As of q1 we had 243000243 million active users on facebook platform and that made up about $30 and revenue per user. In terms of advertising revenue and digging in a little bit more if you just make like a facebook business profile you can see some interesting statistics about that. But you see that the audience is about 200 to 250 million so good. lens up. I've clicked on about 16/30 days post likes 13 and you can see that across america people like coca-cola. That seems to make sense pizza hut judicial watch buffalo wild wings. And i wanted to look at some audiences that i was little bit more familiar with so 26 to 28 year old new york city men and women i noticed that the men were clicking on ads. Fewer times women were slightly. Comedian. On median 350000 to 400,000 women more clicking on ads slightly more than the median for the u.s.. And if you target just looking at people who also like apple you can see that. 80%. Instead of 40% 80% of those people are using iphones. And i'm on man they're clicking on 22 ads now and women are clicking on 27 ads. And if you think that says something about iphone users i tried a lot of different things. Basically just liking anything means are more likely to click on ads. Which creator because you're already more engaged on the platform or because maybe they're being better targeted to you i don't actually know that and i think that's part of the point. And then i wanted to do some background on this audience because i don't know any of you or not that many of you. And it surprised surprised you like our studio in our bloggers that wasn't really that helpful. Focusing on and on new york city i got to know that 40% of. Potentially this audience has a graduate-level degree which probably meant i was never going to impress you with any of my modeling experience also likes it academics day and bam. So. And i think this is important because as you kind of get a better sense of who people are and what their behavior is going to be you can start to think about pushing them in different directions and just taking a page out of richard thaler nudge and when she's talking about choice architecture and how you kind of structure a default choice. I struggle to even answer this question assume you know how to arrange food in the cafeteria knowing that you're going to impact how kids eat. So you can either arrange the food to make the kids best off some ostensibly probably making it healthier for them or tuesday food at random because you hate liability or maybe maximizing sale of items are maximizing profits. But the hardest one was arranging the food to get kids to pick the same choices they would choose themselves. It seems like circular logic cuz i just told you that like the order the food matters. So. That's a really difficult question to answer. And because it's difficult i think that's why people organizations would want to just give the control back to you and say you know what like if this stuff is expensive to you if you don't like this then you can go in and like manually adjust things the way that you want them to you can also download your data. But i think if you don't look at your data and lube are context of how people are targeted you then you don't necessarily have a good sense. Of. What your data is being used for. And so in conclusion i think that modeling behavior is really really difficult and that wasn't because. And that's not necessarily because i think that modeling is always difficult you can make a lot of different models some good or some bad some better some worse. But actually just struggling to define the terms as a society what we think is fair what we think is a choice in what we think is neutral it's really difficult and how much we value privacy vs personalization that's really difficult and i don't think that's going to be determined by like a genius it allowed somewhere but it's actually something that we have to discuss and talk about. And also that impact not intent matters though it's not something that you can hide from. It's something that you have to be proactive about. And more data obviously needs more scrutiny because the more you know the more you kind of have a responsibility to act responsibly with that. And also i feel like. The current environment is very reactive and defensive about these topics. I think it's actually really cool to talk about because we're all learning more and more about human behavior and uncovering where things have been potentially unfair how we could potentially make things better. And so just finishing with this which is a twitter i never knew about before but knowledge is power and now i can be complicated so kudos to people who are comfortably. Keeping it complicated. Thank you. \n"
     ]
    }
   ],
   "source": [
    "path = \"Xv8Abp_1GaM.wav\"\n",
    "text = get_large_audio_transcription(path)\n",
    "print(\"\\nFull text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shreya Gokhe\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectId('5f47283239706096080301d4')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import gridfs\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.test_database  # use a database called \"test_database\"\n",
    "collection = db.files   # and inside that DB, a collection called \"files\"\n",
    "\n",
    "f = open('converted.txt')  # open a file\n",
    "text = f.read()    # read the entire contents, should be UTF-8 text\n",
    "\n",
    "# build a document to be inserted\n",
    "text_file_doc = {\"file_name\": \"converted.txt\", \"contents\" : text }\n",
    "# insert the contents into the \"file\" collection\n",
    "collection.insert(text_file_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TF-IDF algorithm from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-55f2d584fe9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# NLTK function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtotal_documents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sent_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text) # NLTK function\n",
    "total_documents = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_frequency_matrix(sentences):\n",
    "    frequency_matrix = {}\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score_sentences(tf_idf_matrix) -> dict:\n",
    "    \"\"\"\n",
    "    score a sentence by its word's TF\n",
    "    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    sentenceValue = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_score_per_sentence = 0\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, score in f_table.items():\n",
    "            total_score_per_sentence += score\n",
    "\n",
    "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
    "\n",
    "    return sentenceValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_average_score(sentenceValue) -> int:\n",
    "    \"\"\"\n",
    "    Find the average score from the sentence value dictionary\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    sumValues = 0\n",
    "    for entry in sentenceValue:\n",
    "        sumValues += sentenceValue[entry]\n",
    "\n",
    "    # Average value of a sentence from original summary_text\n",
    "    average = (sumValues / len(sentenceValue))\n",
    "\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_summary(sentences, sentenceValue, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-fa702ef74d69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# 8 Find the threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_find_average_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;31m#print(threshold)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-cd7d515f205f>\u001b[0m in \u001b[0;36m_find_average_score\u001b[1;34m(sentenceValue)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Average value of a sentence from original summary_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0maverage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msumValues\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenceValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from nltk.corpus import stopwords    \n",
    "    \n",
    "'''\n",
    "We already have a sentence tokenizer, so we just need \n",
    "to run the sent_tokenize() method to create the array of sentences.\n",
    "'''\n",
    "# 1 Sentence Tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "total_documents = len(sentences)\n",
    "#print(sentences)\n",
    "\n",
    "# 2 Create the Frequency matrix of the words in each sentence.\n",
    "freq_matrix = _create_frequency_matrix(sentences)\n",
    "#print(freq_matrix)\n",
    "\n",
    "'''\n",
    "Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document.\n",
    "'''\n",
    "# 3 Calculate TermFrequency and generate a matrix\n",
    "tf_matrix = _create_tf_matrix(freq_matrix)\n",
    "#print(tf_matrix)\n",
    "\n",
    "# 4 creating table for documents per words\n",
    "count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
    "#print(count_doc_per_words)\n",
    "\n",
    "'''\n",
    "Inverse document frequency (IDF) is how unique or rare a word is.\n",
    "'''\n",
    "# 5 Calculate IDF and generate a matrix\n",
    "idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "#print(idf_matrix)\n",
    "\n",
    "# 6 Calculate TF-IDF and generate a matrix\n",
    "tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "#print(tf_idf_matrix)\n",
    "\n",
    "# 7 Important Algorithm: score the sentences\n",
    "sentence_scores = _score_sentences(tf_idf_matrix)\n",
    "#print(sentence_scores)\n",
    "\n",
    "# 8 Find the threshold\n",
    "threshold = _find_average_score(sentence_scores)\n",
    "#print(threshold)\n",
    "\n",
    "# 9 Important Algorithm: Generate the summary\n",
    "summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using  cosine similarity method of NLTK\n",
    "#### cosine similarity: used to determine how similar the documents are irrespective of their size.\n",
    "#### Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. In this context, the two vectors are arrays containing the word counts of two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords      #Stopwords are the English words which does not add much meaning to a sentence.\n",
    "from nltk.cluster.util import cosine_distance \n",
    "import numpy as np\n",
    "import networkx as nx        #NetworkX is a Python library for studying graphs and networks\n",
    " \n",
    "def read_article(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    filedata = file.readlines()\n",
    "    article = filedata[0].split(\". \")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.pop() \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(file_name, top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "    fh = open(\"summary.txt\", \"w+\")\n",
    "    \n",
    "    # Step 1 - Read text and split it\n",
    "    sentences =  read_article(file_name)\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "\n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "        fh.write(str(summarize_text).strip('[').strip(']'))\n",
    "    # Step 5 - Ofcourse, output the summarize text\n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sometimes if I'm going to catch a flight I actually do use an alarm clock\n",
      "so tired I'm playing and I'll be presenting I'll have what she's having and other models of behavior\n",
      "some background about me I started my career at near economic Consulting in the Securities practice\n",
      "estimating damages and analyzing claims for using litigation\n",
      "this range from\n",
      "Topix\n",
      "such as\n",
      "The Fallout from the financial crisis to product liability to predatory lending\n",
      "and more recently I started at chop't creative salad company and strategy and Analytics\n",
      "and this is my first time going to work with consumers and consumer data so I really wanted to get a better sense of some of the trends in the industry and some of the anxieties that we're all facing today because I'm not only\n",
      "like a data scientist I'm also a person a user and the datapoint\n",
      "so I wanted to be a little bit more cognizant about how I was thinking about that\n",
      "so first we're going to talk about what some models of customer consumer Behavior would be\n",
      "and then I'll\n",
      "tie that to some of my experience working with\n",
      "working with Barbara behavior and some of the intertwined history of discrimination there and also just exploring the Facebook platform because together with Google that accounts for 60% of market share for digital ads in the US with I thought that was a pretty good place to start and also their website apparently has a lot of good hidden fun facts about\n",
      "some of the trends going on\n",
      "so by Prince predictive customer Behavior modeling\n",
      "ultimately I think what organizations are trying to do is segment customers based on their behavior and track them overtime and letting their purchase behavior in order to understand if this is going to be a lucrative customer going forward especially relative to the customer acquisition cost\n",
      "and ultimately it's designed to make a yes no decision about whether or not you're going to engage with that customer\n",
      "this reminded me a lot of\n",
      "models of fire Behavior we're using different factors that borrow represents in the loan application and that ultimately determines whether or not you think that a borrower is going to repay a loan\n",
      "some factors may include credit score loan to value debt to income documentation employment history\n",
      "and ultimately the bank uses those factors to decide whether or not\n",
      "whether or not you're eligible for a loan\n",
      "and if so what\n",
      "what they would charge you for it\n",
      "notably absent on this list are raised familial status another protected categories\n",
      "but I was going forward were becoming more involved with social media you could use factors outside of these outside of these established in\n",
      "loan application factors\n",
      "to predict things going forward\n",
      "also Banks actually pull these loans together across a variety of factors which I think makes sense because if you're relying too much on\n",
      "1 concentrated exposure for example all Coastal\n",
      "people who want beachfront property are all people who work for the Acme car company if there is a bad climate event or bad weather event or\n",
      "there's\n",
      "sales slump in cars\n",
      "then this could have a negative impact to their bottom line\n",
      "and also if there's only one bank or if all the banks are using the same model then customers might\n",
      "The Limited in their choice or even awareness of the offers\n",
      "which is\n",
      "what happened in the history of discrimination redlining\n",
      "in the mortgage Market where it used to be acceptable to say that specific neighborhoods were considered undesirable or risky and this led to selective redlining or\n",
      "underinvestment in minority communities\n",
      "and that had a vicious cyclical effects wear under investment also\n",
      "prevented them from appreciating assassin value\n",
      "and during the Civil Rights Movement the government stepped in\n",
      "and\n",
      "through regulation and\n",
      "the establishment of new agencies\n",
      "was able to\n",
      "try to combat this with new laws and then crucially in 1975 there was also the Home Mortgage disclosure act switch from the cfpb website just want to read requires many financial institutions to maintain Rapport and publicly disclose loan level information about mortgages\n",
      "these data help show whether lenders are serving the housing needs of their communities they give public officials information that helps them make decisions and policies and they sent\n",
      "shed light on Landing patterns that can be discriminatory\n",
      "the public theater modified to protect applicant and borrow privacy\n",
      "I think this is really important because\n",
      "and just taking a public example from\n",
      "the air package handed data can raise a lot of questions so in one example\n",
      "the Boston fed looked into differences\n",
      "in the rate of denial of loan applications between white and minority Borrowers\n",
      "and further collection showed that\n",
      "Berg\n",
      "seller applications\n",
      "for certain\n",
      "this part of this result was related to the underlying factors\n",
      "but for less than Stellar applications that was actually because\n",
      "when you need to compensate\n",
      "compensating factors\n",
      "it was the benefit of the doubt this more often given to White Borrowers\n",
      "and because it's such a nuanced and complicated story I think that's why you continue to see litigation this filled it's hard to Pick-a-Part what's actually an underlying factor in what's actually discriminatory\n",
      "and the complaint will often state that someone was discriminated against relative to a similarly situated whitebark which shows that you yourself might not know when you've been discriminated against for targeted until you have a full data set\n",
      "and I think that you can see him specific categories in this is\n",
      "just hypothetical if you're just using one\n",
      "one category of FICO score to predict something and APR is on the y-axis\n",
      "then you're looking to see if you've been given a different APR based on your nearest neighbor or somebody somebody who has the exact same factors as you and every other way and I think future of personalized experiences can make this especially tricky because you might be different in a lot of different ways and it will be hard to see if you've been treated unfairly\n",
      "and I think this relates to\n",
      "the practice of just allowing people to present the facts and in the gala Terry and fashion\n",
      "so hard recently\n",
      "dude Facebook because they had just been allowing advertisers to Target based on different factors which require elated to the protected classes\n",
      "and\n",
      "unfortunately also drawing red lines around a specific geographies which she can see would be problematic\n",
      "and Facebook develops more of its anti-discriminatory policy and said for housing employment in credit they were actually going to move to a new platform and prevent people from using specific protected classes for targeting\n",
      "and hopefully that this would help alleviate the problem\n",
      "but I think it's also strikes at the heart of the business model because\n",
      "if you're reaching people who matter most to you it and you're using all these different factors they can also be correlated with some of those protected classes and it's actually creating a really complicated environment\n",
      "and\n",
      "I personally have some personal balance between privacy and personalization that I struggle with because sometimes I want to give people my data I don't actually want to\n",
      "get ads that weren't targeted for me and I actually enjoy when there's some personalized Discovery and I find new products that I find relevant\n",
      "and I think that it's also better use of resources overall when people aren't\n",
      "being forced to watch ads that they don't like\n",
      "so it's definitely a balance\n",
      "but I also don't want to live in a world where I feel like my choices are actually being predetermined for me\n",
      "based on who I am\n",
      "can I think that as\n",
      "people are targeting People based on who they think that we are like\n",
      "it could also mean that advertisers are buying for the same users or that everybody's kind of using the same model in the same information in order to push people towards this or that and ultimately it could result in hurting of people are hurting of companies and both ways I think when people are too focused and being pushed in One Singular Direction\n",
      "you might not think about where you're going\n",
      "I'm just taking a little bit more into Facebook active users so\n",
      "as of q1 we had 243000243 million active users on Facebook platform and that made up about $30 and revenue per user\n",
      "in terms of advertising revenue and digging in a little bit more if you just make like a Facebook business profile you can see some interesting statistics about that\n",
      "but you see that the audience is about 200 to 250 million so good\n",
      "Lens up\n",
      "I've clicked on about 16/30 days post likes 13 and you can see that Across America people like Coca-Cola\n",
      "that seems to make sense Pizza Hut Judicial Watch Buffalo Wild Wings\n",
      "and I wanted to look at some audiences that I was little bit more familiar with so 26 to 28 year old New York City men and women I noticed that the men were clicking on ads\n",
      "fewer times women were slightly\n",
      "comedian\n",
      "on median 350000 to 400,000 women more clicking on ads slightly more than the median for the u.s.\n",
      "and if you target just looking at people who also like apple you can see that\n",
      "80%\n",
      "instead of 40% 80% of those people are using iPhones\n",
      "and I'm on man they're clicking on 22 ads now and women are clicking on 27 ads\n",
      "and if you think that says something about iPhone users I tried a lot of different things\n",
      "basically just liking anything means are more likely to click on ads\n",
      "which Creator because you're already more engaged on the platform or because maybe they're being better targeted to you I don't actually know that and I think that's part of the point\n",
      "and then I wanted to do some background on this audience because I don't know any of you or not that many of you\n",
      "and it surprised surprised you like our studio in our bloggers that wasn't really that helpful\n",
      "focusing on and on New York City I got to know that 40% of\n",
      "potentially this audience has a graduate-level degree which probably meant I was never going to impress you with any of my modeling experience also likes it academics day and Bam\n",
      "so\n",
      "and I think this is important because as you kind of get a better sense of who people are and what their behavior is going to be you can start to think about pushing them in different directions and just taking a page out of Richard thaler nudge and when she's talking about Choice architecture and how you kind of structure a default choice\n",
      "I struggle to even answer this question assume you know how to arrange food in the cafeteria knowing that you're going to impact how kids eat\n",
      "so you can either arrange the food to make the kids best off some ostensibly probably making it healthier for them or Tuesday food at random because you hate liability or maybe maximizing sale of items are maximizing profits\n",
      "but the hardest one was arranging the food to get kids to pick the same choices they would choose themselves\n",
      "it seems like circular logic cuz I just told you that like the order the Food Matters\n",
      "so\n",
      "that's a really difficult question to answer\n",
      "and because it's difficult I think that's why people organizations would want to just give the control back to you and say you know what like if this stuff is expensive to you if you don't like this then you can go in and like manually adjust things the way that you want them to you can also download your data\n",
      "but I think if you don't look at your data and Lube are context of how people are targeted you then you don't necessarily have a good sense\n",
      "of\n",
      "what your data is being used for\n",
      "and so in conclusion I think that modeling behavior is really really difficult and that wasn't because\n",
      "and that's not necessarily because I think that modeling is always difficult you can make a lot of different models some good or some bad some better some worse\n",
      "but actually just struggling to define the terms as a society what we think is fair what we think is a choice in what we think is neutral it's really difficult and how much we value privacy vs personalization that's really difficult and I don't think that's going to be determined by like a genius it allowed somewhere but it's actually something that we have to discuss and talk about\n",
      "and also that impact not intent matters though it's not something that you can hide from\n",
      "it's something that you have to be proactive about\n",
      "and more data obviously needs more scrutiny because the more you know the more you kind of have a responsibility to act responsibly with that\n",
      "and also I feel like\n",
      "the current environment is very reactive and defensive about these topics\n",
      "I think it's actually really cool to talk about because we're all learning more and more about human behavior and uncovering where things have been potentially unfair how we could potentially make things better\n",
      "and so just finishing with this which is a Twitter I never knew about before but knowledge is power and now I can be complicated so kudos to people who are comfortably\n",
      "keeping it complicated\n",
      "thank you\n",
      "\n"
     ]
    },
    {
     "ename": "PowerIterationFailedConvergence",
     "evalue": "(PowerIterationFailedConvergence(...), 'power iteration failed to converge within 100 iterations')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPowerIterationFailedConvergence\u001b[0m           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-2b2cf6a376e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerate_summary\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"converted.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-69-181e208450a8>\u001b[0m in \u001b[0;36mgenerate_summary\u001b[1;34m(file_name, top_n)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;31m# Step 3 - Rank sentences in similarity martix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0msentence_similarity_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_similarity_martix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpagerank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_similarity_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;31m# Step 4 - Sort the rank and pick top sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-434>\u001b[0m in \u001b[0;36mpagerank\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\utils\\decorators.py\u001b[0m in \u001b[0;36m_not_implemented_for\u001b[1;34m(not_implement_for_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetworkXNotImplemented\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnot_implement_for_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_not_implemented_for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.py\u001b[0m in \u001b[0;36mpagerank\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPowerIterationFailedConvergence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPowerIterationFailedConvergence\u001b[0m: (PowerIterationFailedConvergence(...), 'power iteration failed to converge within 100 iterations')"
     ]
    }
   ],
   "source": [
    "generate_summary( \"converted.txt\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import gridfs\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.test_database  # use a database called \"test_database\"\n",
    "collection = db.files   # and inside that DB, a collection called \"files\"\n",
    "\n",
    "f = open('summary.txt')  # open a file\n",
    "text = f.read()    # read the entire contents, should be UTF-8 text\n",
    "\n",
    "# build a document to be inserted\n",
    "text_file_doc = {\"file_name\": \"summary.txt\", \"contents\" : text }\n",
    "# insert the contents into the \"file\" collection\n",
    "collection. insert(text_file_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_file_d = collection.find_one({\"file_name\": \"summary.txt\", \"contents\" : text})\n",
    "print(text_file_d['contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS \n",
    "import os \n",
    "import pyttsx3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "No text to speak",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-ddca96b291ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# the module that the converted audio should\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# have a high speed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmyobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgTTS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Saving the converted audio in a mp3 file named\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gtts\\tts.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, tld, lang, slow, lang_check, pre_processor_funcs, tokenizer_func)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;31m# Text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'No text to speak'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: No text to speak"
     ]
    }
   ],
   "source": [
    "# The text that you want to convert to audio \n",
    "  \n",
    "# Language in which you want to convert \n",
    "language = 'en'\n",
    "\n",
    "s = str(text_file_d['contents']).strip('\"')    \n",
    "# Passing the text and language to the engine,  \n",
    "# here we have marked slow=False. Which tells  \n",
    "# the module that the converted audio should  \n",
    "# have a high speed \n",
    "myobj = gTTS(text=s, lang=language) \n",
    "  \n",
    "# Saving the converted audio in a mp3 file named \n",
    "# welcome  \n",
    "myobj.save(\"summ_speech.mp3\") \n",
    "  \n",
    "# Playing the converted file \n",
    "os.system(\"summ_spe.mp3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
